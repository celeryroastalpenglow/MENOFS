# snakemake --resources mem_mb=16000 --executor slurm --default-resources slurm_account=ec85 runtime=120 mem_mb_per_cpu=1000 cpus_per_task=16 --cores 16 --use-conda --keep-going -j 48

configfile: "config/config.yaml"

import itertools

import pandas as pd
import pathlib

date_range=["01-01","12-31"]

# Technologies which will be maximised / minimised for
technologies = ['Solar', 'Windoffshore', 'Windonshore', 'NuclearEPR']
#technologies = ['Solar']
optimisations = ['maximizing', 'minimizing']
#optimisations = ['maximizing']

slacklevels = ['0.05', '0.10', '0.15']
#slacklevels = ['0.05']

cplex_options = """
lpmethod 4
solutiontype 2
threads 16
predual -1
aggind 50
BarColNZ 150
barstartalg 3
BarOrder 3
"""

clusterpath = "/cluster/work/projects/ec85/MENOFS_git/EU10CS/results/"

inputyears = [
    "2010"
]

aggregated_regions = [
    "AT",
    "BE",
    "BG",
    "CH",
    "CZ",
    "DE",
    "DK",
    "EE",
    "ES",
    "FI",
    "FR",
    "UK",
    "GR",
    "HR",
    "HU",
    "IE",
    "IT",
    "LT",
    "LU",
    "LV",
    "NL",
    "NO",
    "PL",
    "PT",
    "RO",
    "SE",
    "SI",
    "SK",
]

# TODO: worry about zones vs regions later this only works for all aggregated
pd.DataFrame(aggregated_regions).rename(columns={0: "zone"}).to_csv(
    "resources/zones.csv", index=None
)
# TODO: maybe at some stage remove the need for this redundant file
pd.DataFrame(aggregated_regions).assign(
    Country="",
    ISO3="",
    ETM=1,
    ETM_reg="",
    EU_simpl="",
    gb_ext="",
    UTC="",
    JRC_EUTIMES="",
).rename(columns={0: "ISO2"}).loc[
    :,
    [
        "Country",
        "ISO3",
        "ISO2",
        "ETM",
        "ETM_reg",
        "EU_simpl",
        "gb_ext",
        "UTC",
        "JRC_EUTIMES",
    ],
].to_csv(
    "resources/europe_countries.csv", index=None
)

spatials = [
    "region",
]

cutoff_values = {"solar": 0.09, "onwind": 0.15, "offwind": 0.20}

scenarios = pd.DataFrame(inputyears, columns=["years"])

concatlist = []
for spatial in spatials:
    concatlist.append(scenarios.assign(spatials=spatial))
scenarios = pd.concat(concatlist).reset_index(drop=True)

years = scenarios.years.to_list()
spatials = scenarios.spatials.to_list()


# in grams per kWh
co2target = round(2, 1)

#gamsfilesha256 = "0e8a55bad6b66243e5af8defd6279a2ce985ed6cf19244b097f08a75d1215c85"
gamsfilesha256 = "0460f672faa655f8495ace9bf2fb98411cc1245bda3d3eae10a7dc909689df7e"
# Get current working directory

cwd = pathlib.Path().resolve()

# Absolute path to your GAMS installation
gamspath = config["paths"]["gams"]
gamspath = str(gamspath)

# Absolute path to the input data that is the same across all versions
shared_input_path = cwd / pathlib.Path(config["paths"]["shared_input"])

# Relative path the model code that is the same across all versions
shared_code_path = "resources/4_model_code_shared/"
abs_shared_code_path = config["paths"]["abs_shared_code"]
# abs_shared_code_path = workflow.source_path("../resources/4_model_code_shared/")
# TODO: Get this to work, maybe with Pathlib

# Relative path to the geodata files that differentiate the scenarios
scenario_exclusions_path = "resources/scenario_exclusions/"

# Relative path to the results
# TODO: Vetle: --directory, flag for Ã¥ sette results directory?
# resultspath = "results/"
resultspath = cwd / pathlib.Path(config["paths"]["results"])
pathlib.Path(resultspath).mkdir(parents=True, exist_ok=True)

# Write scenarios to file, so results analysis script can check if they are all
# available.
scenarios.to_csv(resultspath / "scenarios.csv", sep="\t")

# Relative path to the built model and built inputs
yearonlypart = "models/{year}/"
scenariopart = "{spatial}/"
modelpathyearonly = resultspath / yearonlypart
modelpath = modelpathyearonly / scenariopart
allyearpath = resultspath / "models/allyears/"
logpath = yearonlypart + scenariopart

localrules:
    all,
    build_gams,
    build_shapes,
    build_cplex_opt,
    build_vre_cf_grid,
    build_vre_cf_inputs,
    build_technoeconomic_inputs,
    ensure_gams_template,
    build_vre_land_avail,
    rename_demand_file,
    build_zones_file,
    build_vre_areas_file,
    build_vre_file,
    build_vre_gdx,
    build_hydro_capfac,
    build_hydrores_inflow,
    link_hydrores_inflow,
    convert_results,
    build_vre_parquet,
    build_vre_csv,
    compress_vre_gdx,
    compress_hydrores_inflow,
    build_inputs,
    change_objective,  
    slack_change,
    set_cost_optimal,
    cplex_opt,
    change_objective_EU,
    rule cplex_opt_EU,


rule all:
    input:
        expand(
            clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/results_EU.gdx",
            technology=technologies, 
            optimisation=optimisations, 
            slacklevel=slacklevels,
            year=years,
            spatial=spatials,
            ),

        expand(
            clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/results.gdx",
            technology=technologies, 
            optimisation=optimisations, 
            country=aggregated_regions,
            slacklevel=slacklevels,
            year=years,
            spatial=spatials,
            ),
        

# Phase 1
rule ensure_gams_template:
    input:
        "resources/highresraw.gms",
    output:
        gamsfile=ensure(resultspath / "highres.gms", sha256=gamsfilesha256),
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_gams:
    input:
        rules.ensure_gams_template.output.gamsfile,
    params:
        co2intensity=co2target,
        sharedcodepath=abs_shared_code_path,
    output:
        modelpath / "highres.gms",
    script:
        "scripts/build_gams.py"


rule build_cplex_opt:
    input:
        "resources/cplex.opt",
        #ancient("resources/cplex.opt"), # TODO: What does this line do?
    output:
        modelpath / "cplex.opt",
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_zones_file:
    input:
        "resources/zones.csv",
        "workflow/scripts/build_zones.py",
    output:
        modelpath / "zones.dd",
    script:
        "scripts/build_zones.py"


rule build_technoeconomic_inputs:
	input:
		"resources/zones.csv",
		"resources/gb_ext_scenarios.xls",
		"resources/highres_gb_ext_database.ods",
		europecountriescsvlocation="resources/europe_countries.csv",
		europedemandcsvlocation="resources/europe_demand_1950-2021.csv",
		data2dd="workflow/scripts/data2dd_funcs.py",
	conda:
		"envs/highres_environment.yaml"
	output:
		modelpath / "{year}_temporal.dd",
		modelpath / "BASE_co2_budget.dd",
		modelpath / "BASE_gen.dd",
		modelpath / "BASE_store.dd",
		modelpath / "trans.dd",
		demandfile=temp(modelpath / "BASE_norescale_demand_{year}.dd"),
	params:
		aggregated_regions=aggregated_regions,
		date_range=date_range,
	script:
		"scripts/gb_ext_data2dd.py"


rule rename_demand_file:
    input:
        rules.build_technoeconomic_inputs.output.demandfile,
    output:
        modelpath / "BASE_demand_{year}.dd",
    run:
        import shutil

        shutil.copy2(input[0], output[0])
		
rule build_shapes:
	input:
		euroshape=shared_input_path
		/ "geodata/onshore/shapes/NUTS_RG_01M_2021_4326.geojson",
		eurooffshoreshape=shared_input_path
		/ ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
	conda:
		"envs/highres_environment.yaml"
	output:
		onshoreshape= resultspath / "intermediate_data/{spatial}/shapes/europe_onshore.geojson",
		offshoreshape= resultspath / "intermediate_data/{spatial}/shapes/europe_offshore.geojson",
	params:
		aggregated_regions=aggregated_regions,
	notebook:
		"notebooks/highRES-build_shapes.ipynb"
        

rule build_vre_cf_grid:
	input:
		weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
		euroshape=rules.build_shapes.output.onshoreshape,
		eurooffshoreshape=rules.build_shapes.output.offshoreshape,
	conda:
		"envs/highres_environment.yaml"
	output:
		cf_file= resultspath / "intermediate_data/{spatial}/weather/europe_cf_{year}.nc",
	resources:
		mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
	threads: workflow.cores * 0.75
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
	notebook:
		"notebooks/highRES-build_vre_cf_grid.ipynb"
		
rule build_vre_land_avail:
	input:
		WDPA1a=shared_input_path / "geodata/onshore/WDPA_Ia_100.tiff",
		WDPA1b=shared_input_path / "geodata/onshore/WDPA_Ib_100.tiff",
		WDPA2=shared_input_path / "geodata/onshore/WDPA_II_100.tiff",
		WDPA3=shared_input_path / "geodata/onshore/WDPA_III_100.tiff",
		WDPA4=shared_input_path / "geodata/onshore/WDPA_IV_100.tiff",
		elevation=shared_input_path / "geodata/onshore/2000m.shp.zip",
		slope=shared_input_path / "geodata/onshore/15degrees.shp.zip",
		euroshape=rules.build_shapes.output.onshoreshape,
		eurooffshoreshape=rules.build_shapes.output.offshoreshape,
		weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
		cfdata=rules.build_vre_cf_grid.output.cf_file,
		corine=shared_input_path / "geodata/onshore/corine.tif",
	conda:
		"envs/highres_environment.yaml"
	output:
        # FIXME the file indreg is only necessary when we run on grid cell
        # currently when we run with region, we create an empty file
        # the cleaner solution might be to outsource the creation of that file
        # to an extra rule and have an input function conditional on the
        # wildcard spatial for that rule
		cf_exclusion_solar=temp(modelpath / "cf_exclusion_solar.tif"),
		cf_exclusion_windon=temp(modelpath / "cf_exclusion_windon.tif"),
		cf_exclusion_windoff=temp(modelpath / "cf_exclusion_windoff.tif"),
		indreg=temp(modelpath / "indices_region.csv"),
		grid_areassolar = modelpath / "grid_areas_solar.csv",
		grid_areaswindonshore=modelpath / "grid_areas_wind_onshore.csv",
		grid_areaswindoffshore=modelpath / "grid_areas_wind_offshore.csv",
	resources:
		mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
	threads: workflow.cores * 0.75
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
		cutoffs=cutoff_values,
	notebook:
		"notebooks/highRES-build_vre_land_avail.ipynb"
		
rule build_vre_cf_inputs:
	input:
		cf_file=rules.build_vre_cf_grid.output.cf_file,
		grid_areassolar = rules.build_vre_land_avail.output.grid_areassolar,
		grid_areaswindonshore = rules.build_vre_land_avail.output.grid_areaswindonshore,
		grid_areaswindoffshore = rules.build_vre_land_avail.output.grid_areaswindoffshore,
	conda:
		"envs/highres_environment.yaml"
	output:
		capfacfile=temp(modelpath / "capacity-factors_solar_{year}.csv"),
		areassolar=temp(modelpath / "areas_solar.csv"),
		areaswindonshore=temp(modelpath / "areas_wind_onshore.csv"),
		areaswindoffshore=temp(modelpath / "areas_wind_offshore.csv"),
	params:
		date_range=date_range,
	notebook:
		"notebooks/highRES-build_vre_cf_inputs.ipynb"



rule build_hydro_capfac:
	input:
		eiahydrogen="resources/EIA_hydro_generation_1995_2000_2014.csv",
		hydroinstalledcap="resources/hydro_installed_cap.tsv",
		euroshape=rules.build_shapes.output.onshoreshape,
		weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
	output:
		hydrororcapfac=temp(modelpath / "capacity-factors_hydro_{year}.csv"),
		hydroresinfl=temp(modelpath / "inflow_hydro-res_{year}.csv"),
		areashydro=temp(modelpath / "hydro_ror_area.csv"),
	conda:
		"envs/highres_environment.yaml"
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
		date_range=date_range,
	notebook:
		"notebooks/highRES_build_hydro.py.ipynb"


def regionsonlyforgrid(wildcards):
    returndict = {}
    if wildcards.spatial == "grid":
        returndict["indreg"] = rules.build_vre_land_avail.output.indreg
    return returndict


# def build_vre_areas_file_func(wildcards):
#    returndict = {}
#    if wildcards.neigh != "Extreme":
#        returndict['areaswindon'] = rules.build_weather.output.areaswindonshore
#    return returndict


rule build_vre_areas_file:
    input:
        areashydro=rules.build_hydro_capfac.output.areashydro,
        areassolar=rules.build_vre_cf_inputs.output.areassolar,
        areaswindon=rules.build_vre_cf_inputs.output.areaswindonshore,
        areaswindoff=rules.build_vre_cf_inputs.output.areaswindoffshore,
        data2dd=pathlib.Path(workflow.basedir) / "scripts",
    conda:
        "envs/highres_environment.yaml"
    output:
        modelpath / "vre_areas_{year}_.dd",
        regionsdd=modelpath / "_regions.dd",
    notebook:
        "notebooks/highRES_build_vre_areas_file.ipynb"


# shell:
#     # TODO platform independence
#     (
#         "cat {input[areashydro]} {input[areassolar]} {input[areaswindon]} "
#         "{input[areaswindoff]} | sort -g | cat {input[vreareaheader]} - "
#         "{input[genericfooter]} > {output[0]}"
#     )
# """ run:
#     import shutil
#     with open('unsorted.txt','wb') as wfd:
#         for f in [input[areashydro],input[areassolar],input[areaswindon],input[areaswindoff]]:
#             with open(f,'rb') as fd:
#                 shutil.copyfileobj(fd, wfd) """


rule build_hydrores_inflow:
    input:
        rules.build_hydro_capfac.output.hydroresinfl,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    shell:
        gamspath + (
            "csv2gdx {input} output={output} ID=hydro_inflow "
            "Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_hydrores_inflow:
    input:
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        compressdone=touch(modelpathyearonly / "hydrores_inflow_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


rule link_hydrores_inflow:
    input:
        rules.compress_hydrores_inflow.output.compressdone,
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    run:
        # copy instead of link due to missing privileges on windows
        import shutil

        shutil.copy2(input[1], output[0])


rule build_vre_file:
    input:
        cfwsng=rules.build_vre_cf_inputs.output.capfacfile,
        cfhn3=rules.build_hydro_capfac.output.hydrororcapfac,
    output:
        vrefile=temp(modelpath / "vre_{year}_.csv"),
    run:
        import shutil

        with open(output.vrefile, "wb") as wfd:
            for f in [
                input["cfwsng"],
                input["cfhn3"],
            ]:
                with open(f, "rb") as fd:
                    shutil.copyfileobj(fd, wfd)
                    # shell:
                    #     # TODO platform independence
                    #     (
                    #         "cat {input[cfwsng]} {input[cfhn3]} | sed 's/bottom//'"
                    #         " > {output[vrefile]}"
                    #     )



rule build_vre_parquet:
    input:
        rules.build_vre_file.output.vrefile,
    output:
        modelpath / "vre_{year}_.parquet",
    conda:
        "envs/highres_environment.yaml"
    script:
        "scripts/build_vre_parquet.py"


rule build_vre_csv:
    input:
        modelpath / "vre_{year}_.parquet",
    output:
        csvgdx=temp(modelpath / "vre_{year}_tmp.csv"),
    conda:
        "envs/highres_environment.yaml"
    script:
        "scripts/build_vre_csv.py"


rule build_vre_gdx:
    input:
        rules.build_vre_csv.output.csvgdx,
    output:
        bigvregdx=temp(modelpath / "vre_{year}_.gdx"),
    shell:
        # TODO platform independence
        gamspath + (
            "csv2gdx {input} output={output} ID=vre_gen Index='(1,2,3)'"
            " Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_vre_gdx:
    input:
        rules.build_vre_gdx.output.bigvregdx,
    output:
        touch(modelpath / "vre_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


""" def inputfilelist(wildcards):
    returndict = {}
    returndict['regionsfile'] = modelpath / "_regions.dd"
    returndict['zonesfile'] = modelpath / "zones.dd"
    returndict['temporalfile'] = modelpath / "{year}_temporal.dd"
    returndict['vreareafile'] = modelpath / "vre_areas_{year}_.dd"
    returndict['demandfile'] = modelpath / "BASE_demand_{year}.dd"
    returndict['capfacfilecompressed'] = modelpath / "vre_gdx.compressed"
    returndict['capfacfile'] = modelpath / "vre_{year}_.gdx"
    returndict['hydroresinflowfile'] = modelpath / "hydro_res_inflow_{year}.gdx"
    returndict['co2budgetfile'] = modelpath / "BASE_co2_budget.dd"
    returndict['genparamsfile'] = modelpath / "BASE_gen.dd"
    returndict['storeparamsfile'] = modelpath / "BASE_store.dd"
    returndict['transparamsfile'] = modelpath / "trans.dd"
    return returndict """


rule build_inputs:
    input:
        # unpack(inputfilelist)
        modelpath / "_regions.dd",
        modelpath / "zones.dd",
        modelpath / "{year}_temporal.dd",
        modelpath / "vre_areas_{year}_.dd",
        modelpath / "BASE_demand_{year}.dd",
        modelpath / "vre_gdx.compressed",
        modelpath / "vre_{year}_.gdx",
        modelpath / "hydro_res_inflow_{year}.gdx",
        modelpath / "BASE_co2_budget.dd",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
    output:
        touch(modelpath / "inputs.finished"),


rule run_model:
    input:
        modelpath / "highres.gms",
        modelpath / "cplex.opt",
        shared_code_path + "highres_data_input.gms",
        shared_code_path + "highres_hydro.gms",
        shared_code_path + "highres_results.gms",
        shared_code_path + "highres_storage_setup.gms",
        shared_code_path + "highres_storage_uc_setup.gms",
        shared_code_path + "highres_uc_setup.gms",
        modelpath / "inputs.finished",
        modelpath / "vre_{year}_.gdx",
    params:
        gamspath=gamspath,
        modelpath=str(modelpath),
    # retries: 3
    log:
        str(modelpath) + "/highres.lst",
        str(modelpath) + "/highres.log",
    output:
        modelresults=modelpath / "results.gdx",
        #modelresultsdd=protected(modelpath / "results.db"),
    script:
        # TODO platform independence
        "scripts/run_gams.sh"


rule convert_results:
    input:
        rules.run_model.output.modelresults,
    output:
        resultsdb=ensure(protected(modelpath / "results.db"), non_empty=True),
    shell:
        # TODO platform independence
        gamspath + "gdx2sqlite -i {input} -o {output} -fast"

# Phase 2
rule set_cost_optimal:
    input:
        rules.convert_results.output.resultsdb,
    output:
        cost_opt=modelpath / "cost_optimal.tsv",
    script:
        "scripts/set_cost_optimal.py"

# This rule changes the slack level
rule slack_change:
    input:
        "resources/mga_parameters.dd",
        rules.set_cost_optimal.output.cost_opt,
    output:
        mgaparams=clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/mga_parameters.dd"
    script:
        "scripts/change_slack.py"

# This rule takes the blueprint MGA .gms file and changes the objective for the different wildcards.
rule change_objective:
	input:
		"resources/highres_mga.gms",
		rules.slack_change.output.mgaparams,
	output:
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/highres_mga.gms"
	script:
		"scripts/change_MGA_objective.py"

# This rule copies over model code to cluster
rule copy_model_code:
	input:
		"resources/4_model_code_shared"
	output:
		directory(clusterpath + "4_model_code_shared")
	shell:
		"cp -r resources/4_model_code_shared/ /cluster/work/projects/ec85/MENOFS_git/EU10CS/results/4_model_code_shared"

rule cplex_opt:
	output:
		cplex_mga = clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/cplex.opt"
	run:
		with open(output[0], "w+") as f:
			f.writelines(cplex_options)

# This rule runs the MGA stuff
rule run_mga:
	input:
		#modelpath / "cplex.opt",
		rules.cplex_opt.output.cplex_mga,
		shared_code_path + "highres_data_input.gms",
		shared_code_path + "highres_hydro.gms",
		shared_code_path + "highres_results.gms",
		shared_code_path + "highres_storage_setup.gms",
		shared_code_path + "highres_storage_uc_setup.gms",
		shared_code_path + "highres_uc_setup.gms",
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/mga_parameters.dd",
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/highres_mga.gms",
		modelpath / "vre_{year}_.gdx",
		"/fp/projects01/ec85/.bin/gams/gams41.5_linux_x64_64_sfx",
	params:
		gamspath=gamspath,
		modelpath=str(modelpath),
	log:
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/highres_mga.lst",
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/highres_mga.log",
	output:
		clusterpath + "{year}_{spatial}_{country}_{technology}_{optimisation}_{slacklevel}/results.gdx"
	script:
		"scripts/run_gams_mga.sh"

# Phase 3
# One rule to change the .gms file
rule change_objective_EU:
	input:
		"resources/highres_mga.gms",
		#rules.slack_change.output.mgaparams,
	output:
		clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/highres_mga_EU.gms"
	script:
		"scripts/change_MGA_objective_EU.py"

rule cplex_opt_EU:
	output:
		cplex_mga = clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/cplex.opt"
	run:
		with open(output[0], "w+") as f:
			f.writelines(cplex_options)

# One rule to run the MGA stuff
rule run_mga_EU:
	input:
		rules.cplex_opt_EU.output.cplex_mga,
		shared_code_path + "highres_data_input.gms",
		shared_code_path + "highres_hydro.gms",
		shared_code_path + "highres_results.gms",
		shared_code_path + "highres_storage_setup.gms",
		shared_code_path + "highres_storage_uc_setup.gms",
		shared_code_path + "highres_uc_setup.gms",
		clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/mga_parameters.dd",
		clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/highres_mga_EU.gms",
		modelpath / "vre_{year}_.gdx",
		"/fp/projects01/ec85/.bin/gams/gams41.5_linux_x64_64_sfx",
	params:
		gamspath=gamspath,
		modelpath=str(modelpath),
	output:
		clusterpath + "{year}_{spatial}_EU_{technology}_{optimisation}_{slacklevel}/results_EU.gdx",
	script:
		"scripts/run_gams_mga_EU.sh"
